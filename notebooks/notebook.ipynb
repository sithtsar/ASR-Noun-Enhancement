{
  "cells": [
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sithtsar/ASR-Noun-Enhancement/blob/main/notebooks/notebook.ipynb)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "# ASR Noun Enhancement Assignment\n",
    "Implementation of spell correction for ASR noun enhancement using baseline and advanced models.\n",
    "\n",
     "This notebook covers:\n",
     "- Data loading and preprocessing\n",
     "- Exploratory Data Analysis (EDA) with visualizations\n",
     "- Error extraction and categorization\n",
     "- Baseline model (dictionary + Levenshtein + trigram)\n",
     "- Advanced model (T5 fine-tuning)\n",
     "- Evaluation and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (for Colab)\n",
    "!pip install pandas numpy scikit-learn nltk transformers torch datasets sentencepiece matplotlib spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from difflib import SequenceMatcher\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(data_path: str, extension_pattern: str) -> pd.DataFrame:\n",
    "    data_dir = Path.cwd() / data_path\n",
    "    data_file_extsn = f\"*.{extension_pattern}\"\n",
    "    data_file_path = list(data_dir.glob(data_file_extsn))[0]\n",
    "    df = pd.read_excel(data_file_path)\n",
    "    return df\n",
    "\n",
    "df = load_df(\"data\", \"xlsx\")\n",
    "print(\"Dataset loaded:\")\n",
    "print(df.head())\n",
    "print(f\"Total samples: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def load_spacy_model():\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        spacy.cli.download(\"en_core_web_sm\")\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    return nlp\n",
    "\n",
    "nlp = load_spacy_model()\n",
    "\n",
    "def extract_nouns_ner(text: str) -> list:\n",
    "    doc = nlp(text)\n",
    "    nouns = [ent.text for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'GPE', 'PRODUCT']]\n",
    "    return nouns\n",
    "\n",
    "def extract_nouns_pos(text: str) -> list:\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    nouns = [word for word, tag in tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "    return nouns\n",
    "\n",
    "# Apply preprocessing\n",
    "df['clean_correct'] = df['correct sentences'].apply(clean_text)\n",
    "df['clean_incorrect'] = df['ASR-generated incorrect transcriptions'].apply(clean_text)\n",
    "df['nouns_correct_ner'] = df['correct sentences'].apply(extract_nouns_ner)\n",
    "df['nouns_incorrect_ner'] = df['ASR-generated incorrect transcriptions'].apply(extract_nouns_ner)\n",
    "df['nouns_correct_pos'] = df['correct sentences'].apply(extract_nouns_pos)\n",
    "df['nouns_incorrect_pos'] = df['ASR-generated incorrect transcriptions'].apply(extract_nouns_pos)\n",
    "\n",
    "print(\"Preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic stats\n",
    "df['len_correct'] = df['clean_correct'].apply(lambda x: len(x.split()))\n",
    "df['len_incorrect'] = df['clean_incorrect'].apply(lambda x: len(x.split()))\n",
    "\n",
    "stats = {\n",
    "    'total_samples': len(df),\n",
    "    'avg_len_correct': df['len_correct'].mean(),\n",
    "    'avg_len_incorrect': df['len_incorrect'].mean(),\n",
    "    'vocab_size_correct': len(set(' '.join(df['clean_correct']).split())),\n",
    "    'vocab_size_incorrect': len(set(' '.join(df['clean_incorrect']).split())),\n",
    "    'avg_nouns_ner': df['nouns_correct_ner'].apply(len).mean(),\n",
    "    'avg_nouns_pos': df['nouns_correct_pos'].apply(len).mean()\n",
    "}\n",
    "\n",
    "print(\"Stats:\", json.dumps(stats, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['len_correct'], alpha=0.5, label='Correct')\n",
    "plt.hist(df['len_incorrect'], alpha=0.5, label='Incorrect')\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sentence Length Distribution')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Noun count vs length\n",
    "plt.scatter(df['len_correct'], df['nouns_correct_pos'].apply(len))\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Noun Count')\n",
    "plt.title('Nouns vs Sentence Length')\n",
    "plt.show()"
   ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 2.5 Additional EDA Plots"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# Load processed data for additional plots\n",
     "import json\n",
     "from collections import Counter\n",
     "import ast\n",
     "\n",
     "# Assuming processed data exists; if not, run the scripts first\n",
     "try:\n",
     "    processed_df = pd.read_csv('data/processed/augmented_df.csv')\n",
     "    with open('data/processed/stats.json', 'r') as f:\n",
     "        stats = json.load(f)\n",
     "    print(\"Loaded processed data.\")\n",
     "except FileNotFoundError:\n",
     "    print(\"Processed data not found. Run preprocessing scripts first.\")\n",
     "    processed_df = None\n",
     "    stats = None\n",
     "\n",
     "if processed_df is not None:\n",
     "    # Error types plot\n",
     "    counters = processed_df['error_categories'].dropna().apply(ast.literal_eval)\n",
     "    total = Counter()\n",
     "    for c in counters:\n",
     "        total.update(c)\n",
     "    plt.figure(figsize=(10, 6))\n",
     "    plt.bar(list(total.keys()), list(total.values()))\n",
     "    plt.xlabel('Error Type')\n",
     "    plt.ylabel('Frequency')\n",
     "    plt.title('Frequency of Error Types')\n",
     "    plt.show()\n",
     "    \n",
     "    # Error distribution\n",
     "    plt.figure(figsize=(10, 6))\n",
     "    plt.hist(processed_df['num_errors'], bins=range(0, processed_df['num_errors'].max()+2), alpha=0.7)\n",
     "    plt.xlabel('Number of Errors per Sentence')\n",
     "    plt.ylabel('Frequency')\n",
     "    plt.title('Distribution of Errors per Sentence')\n",
     "    plt.show()\n",
     "    \n",
     "    # Vocab stats pie\n",
     "    if stats:\n",
     "        labels = ['Correct Vocab', 'Incorrect Vocab', 'Medical Correct', 'Medical Incorrect']\n",
     "        sizes = [stats['vocab_size_correct'], stats['vocab_size_incorrect'], stats['medical_terms_correct'], stats['medical_terms_incorrect']]\n",
     "        plt.figure(figsize=(8, 8))\n",
     "        plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
     "        plt.title('Vocabulary Coverage Statistics')\n",
     "        plt.show()\n",
     "    \n",
     "    # Length impact\n",
     "    plt.figure(figsize=(10, 6))\n",
     "    plt.scatter(processed_df['len_correct'], processed_df['num_errors'])\n",
     "    plt.xlabel('Sentence Length (words)')\n",
     "    plt.ylabel('Number of Errors')\n",
     "    plt.title('Context Length Impact on Correction Errors')\n",
     "    plt.show()\n",
     "    \n",
     "    # Noun error distribution\n",
     "    plt.figure(figsize=(10, 6))\n",
     "    plt.scatter(processed_df['nouns_correct_pos'].apply(len), processed_df['num_errors'])\n",
     "    plt.xlabel('Number of Nouns in Sentence')\n",
     "    plt.ylabel('Number of Errors')\n",
     "    plt.title('Noun Count vs Errors')\n",
     "    plt.show()"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 3. Error Extraction and Categorization"
    ]
   },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def get_differing_words(correct, incorrect):\n",
    "    correct_words = correct.split()\n",
    "    incorrect_words = incorrect.split()\n",
    "    matcher = SequenceMatcher(None, correct_words, incorrect_words)\n",
    "    blocks = matcher.get_matching_blocks()\n",
    "    errors = []\n",
    "    i = 0\n",
    "    for block in blocks:\n",
    "        for j in range(i, block.a):\n",
    "            if block.b > 0:\n",
    "                errors.append((correct_words[j], incorrect_words[block.b - (block.a - j)]))\n",
    "        i = block.a + block.size\n",
    "    return errors\n",
    "\n",
    "def categorize_error(incorrect, correct):\n",
    "    if not incorrect or not correct:\n",
    "        return 'segmentation'\n",
    "    matcher = SequenceMatcher(None, incorrect, correct)\n",
    "    dist = len(incorrect) + len(correct) - 2 * sum(block.size for block in matcher.get_matching_blocks())\n",
    "    if dist <= 2:\n",
    "        return 'character'\n",
    "    elif len(incorrect.split()) != len(correct.split()):\n",
    "        return 'segmentation'\n",
    "    elif matcher.ratio() > 0.6:\n",
    "        return 'phonetic'\n",
    "    else:\n",
    "        return 'word'\n",
    "\n",
    "# Apply\n",
    "df['error_words'] = df.apply(lambda row: get_differing_words(row['clean_correct'], row['clean_incorrect']), axis=1)\n",
    "df['error_categories'] = df['error_words'].apply(lambda x: [categorize_error(inc, cor) for inc, cor in x if inc and cor])\n",
    "\n",
    "# Error DB\n",
    "from collections import defaultdict\n",
    "error_db = defaultdict(list)\n",
    "for errors in df['error_words']:\n",
    "    for cor, inc in errors:\n",
    "        if inc and cor:\n",
    "            error_db[inc].append(cor)\n",
    "final_db = {inc: Counter(cors).most_common(1)[0][0] for inc, cors in error_db.items()}\n",
    "\n",
    "print(\"Error DB built with\", len(final_db), \"entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "def build_trigram_model(sentences):\n",
    "    model = {}\n",
    "    for sent in sentences:\n",
    "        tokens = word_tokenize(sent)\n",
    "        for i in range(len(tokens)-2):\n",
    "            prev, mid, next_t = tokens[i], tokens[i+1], tokens[i+2]\n",
    "            if prev not in model:\n",
    "                model[prev] = {}\n",
    "            if mid not in model[prev]:\n",
    "                model[prev][mid] = {}\n",
    "            if next_t not in model[prev][mid]:\n",
    "                model[prev][mid][next_t] = 0\n",
    "            model[prev][mid][next_t] += 1\n",
    "    return model\n",
    "\n",
    "trigram_model = build_trigram_model(df['clean_correct'])\n",
    "\n",
    "def correct_sentence(sentence, error_db, trigram_model):\n",
    "    words = sentence.split()\n",
    "    corrected = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word in error_db:\n",
    "            corrected.append(error_db[word])\n",
    "        else:\n",
    "            candidates = get_close_matches(word, list(trigram_model.keys()), n=3)\n",
    "            if candidates:\n",
    "                corrected.append(candidates[0])\n",
    "            else:\n",
    "                corrected.append(word)\n",
    "    return ' '.join(corrected)\n",
    "\n",
    "# Test on sample\n",
    "sample = df['clean_incorrect'].iloc[0]\n",
    "corrected = correct_sentence(sample, final_db, trigram_model)\n",
    "print(f\"Original: {sample}\")\n",
    "print(f\"Corrected: {corrected}\")\n",
    "print(f\"Ground truth: {df['clean_correct'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Model (T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_dataset = Dataset.from_dict({'input_text': train_df['clean_incorrect'].tolist(), 'target_text': train_df['clean_correct'].tolist()})\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['input_text'], max_length=128, truncation=True, padding='max_length')\n",
    "    targets = tokenizer(examples['target_text'], max_length=128, truncation=True, padding='max_length')\n",
    "    inputs['labels'] = targets['input_ids']\n",
    "    return inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Training (quick for demo)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset)\n",
    "trainer.train()\n",
    "\n",
    "print(\"T5 training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(df, model_func):\n",
    "    predictions = [model_func(row['clean_incorrect']) for _, row in df.iterrows()]\n",
    "    references = df['clean_correct'].tolist()\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = sum(1 for p, r in zip(predictions, references) if p == r) / len(df)\n",
    "    \n",
    "    # BLEU\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu = sum(sentence_bleu([r.split()], p.split(), smoothing_function=smoothie) for p, r in zip(predictions, references)) / len(df)\n",
    "    \n",
    "    # Noun accuracy\n",
    "    def get_nouns(sent):\n",
    "        return set(extract_nouns_pos(sent))\n",
    "    \n",
    "    correct_nouns = [get_nouns(r) for r in references]\n",
    "    pred_nouns = [get_nouns(p) for p in predictions]\n",
    "    noun_acc = sum(len(c & p) / len(c) if c else 0 for c, p in zip(correct_nouns, pred_nouns)) / len(df)\n",
    "    \n",
    "    return accuracy, bleu, noun_acc\n",
    "\n",
    "# Baseline eval\n",
    "baseline_func = lambda x: correct_sentence(x, final_db, trigram_model)\n",
    "acc_b, bleu_b, noun_b = evaluate_model(test_df, baseline_func)\n",
    "print(f\"Baseline: Acc {acc_b:.4f}, BLEU {bleu_b:.4f}, Noun {noun_b:.4f}\")\n",
    "\n",
    "# Advanced eval\n",
    "def t5_correct(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', max_length=128, truncation=True).to(device)\n",
    "    outputs = model.generate(**inputs, max_length=128)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "acc_a, bleu_a, noun_a = evaluate_model(test_df.head(10), t5_correct)  # sample for speed\n",
    "print(f\"Advanced: Acc {acc_a:.4f}, BLEU {bleu_a:.4f}, Noun {noun_a:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison plot\n",
    "metrics = ['Accuracy', 'BLEU', 'Noun Accuracy']\n",
    "baseline_vals = [acc_b, bleu_b, noun_b]\n",
    "advanced_vals = [acc_a, bleu_a, noun_a]\n",
    "\n",
    "x = range(len(metrics))\n",
    "plt.bar(x, baseline_vals, width=0.4, label='Baseline')\n",
    "plt.bar([i+0.4 for i in x], advanced_vals, width=0.4, label='Advanced')\n",
    "plt.xticks([i+0.2 for i in x], metrics)\n",
    "plt.legend()\n",
    "plt.title('Model Comparison')\n",
    "plt.show()\n",
    "\n",
    "print(\"Notebook completed. For full pipeline, run the scripts.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}